{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i94wgujkh7xn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user3/miniconda3/envs/venv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-05-01 10:45:22.269989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.set_memory_growth\n",
        "import os\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n",
        "device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BTESt9Fk4pa",
        "outputId": "18817e45-7808-4fe7-a4e2-60f1e13c19dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27481 3534\n"
          ]
        }
      ],
      "source": [
        "train_set = pd.read_csv(\"train.csv\", encoding= 'unicode_escape', sep = ',')\n",
        "test_set = pd.read_csv(\"test.csv\", encoding= 'unicode_escape', sep = ',')\n",
        "train_set = train_set.fillna('')\n",
        "test_set = test_set[:3534]  #trailing empty rows\n",
        "print(len(train_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qkbVFQ4y2su8"
      },
      "outputs": [],
      "source": [
        "X_train = train_set['text']\n",
        "y_train = train_set['sentiment']\n",
        "y_train = y_train.replace(['negative','neutral','positive'],[0,1,2]) \n",
        "X_test = test_set['text']\n",
        "y_test = test_set['sentiment']\n",
        "y_test = y_test.replace(['negative','neutral','positive'],[0,1,2]) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1o3XfqfoXmX",
        "outputId": "f139f546-2df8-4f2b-d753-9c00813c9a0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3, output_attentions = False, output_hidden_states = False)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l0wkJVB7kBDO"
      },
      "outputs": [],
      "source": [
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = 50,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask = True\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = 50,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nFw9Skjs4TKa"
      },
      "outputs": [],
      "source": [
        "train_seq = torch.tensor(train_tokens['input_ids'])\n",
        "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
        "train_y = torch.tensor(y_train.tolist())\n",
        "\n",
        "test_seq = torch.tensor(test_tokens['input_ids'])\n",
        "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
        "test_y = torch.tensor(y_test.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EAvC-Nd22hWS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "McnKIt6f7axu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "bert.to(device)\n",
        "print(bert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p3Nlje_59Exl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(bert.parameters(), lr = 5e-5, eps = 1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HnRTY63O-QQL"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dtHQaO82-l5U"
      },
      "outputs": [],
      "source": [
        "def accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQYfRHR88WBg",
        "outputId": "e71674d5-be30-4a4e-a71a-97eb33180493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    859.\n",
            "  Batch   200  of    859.\n",
            "  Batch   300  of    859.\n",
            "  Batch   400  of    859.\n",
            "  Batch   500  of    859.\n",
            "  Batch   600  of    859.\n",
            "  Batch   700  of    859.\n",
            "  Batch   800  of    859.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.51\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    859.\n",
            "  Batch   200  of    859.\n",
            "  Batch   300  of    859.\n",
            "  Batch   400  of    859.\n",
            "  Batch   500  of    859.\n",
            "  Batch   600  of    859.\n",
            "  Batch   700  of    859.\n",
            "  Batch   800  of    859.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.52\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    859.\n",
            "  Batch   200  of    859.\n",
            "  Batch   300  of    859.\n",
            "  Batch   400  of    859.\n",
            "  Batch   500  of    859.\n",
            "  Batch   600  of    859.\n",
            "  Batch   700  of    859.\n",
            "  Batch   800  of    859.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.66\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    859.\n",
            "  Batch   200  of    859.\n",
            "  Batch   300  of    859.\n",
            "  Batch   400  of    859.\n",
            "  Batch   500  of    859.\n",
            "  Batch   600  of    859.\n",
            "  Batch   700  of    859.\n",
            "  Batch   800  of    859.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.92\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "training_stats = []\n",
        "best = float('-inf')\n",
        "# Measure the total training time for the whole run.\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. \n",
        "    bert.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        # \n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Clear Gradients\n",
        "        bert.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch)\n",
        "        output = bert(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        loss, logits = output.loss, output.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    bert.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            output = bert(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "            loss, logits = output.loss, output.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
        "    if avg_test_accuracy > best:\n",
        "        torch.save(bert.state_dict(), 'weights.pt')\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_eval_loss / len(test_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_test_loss))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': avg_test_accuracy,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p-I7NQA193lF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "path = 'weights.pt'\n",
        "bert.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.79\n"
          ]
        }
      ],
      "source": [
        "bert.eval()\n",
        "\n",
        "total_eval_accuracy = 0\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    \n",
        "    with torch.no_grad():        \n",
        "\n",
        "        output = bert(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        \n",
        "        logits = output.logits\n",
        "        \n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    total_eval_accuracy += accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv2",
      "language": "python",
      "name": "venv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
