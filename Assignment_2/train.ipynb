{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import mlflow\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    return [word.lemma for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    acc = accuracy_score(actual, pred)\n",
    "    f1 = f1_score(actual, pred, pos_label='spam')\n",
    "    auc = roc_auc_score(actual=='spam', pred=='spam')\n",
    "    return acc, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For dataset generated by seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/train.csv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, quoting\u001b[39m=\u001b[39mcsv\u001b[39m.\u001b[39mQUOTE_NONE, index_col\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m bow_transformer \u001b[39m=\u001b[39m CountVectorizer(analyzer\u001b[39m=\u001b[39msplit_into_lemmas)\u001b[39m.\u001b[39mfit(messages[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m messages_bow \u001b[39m=\u001b[39m bow_transformer\u001b[39m.\u001b[39;49mtransform(messages[\u001b[39m'\u001b[39;49m\u001b[39mmessage\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      4\u001b[0m tfidf_transformer \u001b[39m=\u001b[39m TfidfTransformer()\u001b[39m.\u001b[39mfit(messages_bow)\n\u001b[1;32m      5\u001b[0m messages_tfidf \u001b[39m=\u001b[39m tfidf_transformer\u001b[39m.\u001b[39mtransform(messages_bow)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1432\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1431\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1434\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1273\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1275\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    106\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m, in \u001b[0;36msplit_into_lemmas\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_into_lemmas\u001b[39m(message):\n\u001b[1;32m      2\u001b[0m     message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mlower()\n\u001b[0;32m----> 3\u001b[0m     words \u001b[39m=\u001b[39m TextBlob(message)\u001b[39m.\u001b[39;49mwords\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [word\u001b[39m.\u001b[39mlemma \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m---> 24\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(obj)\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/blob.py:678\u001b[0m, in \u001b[0;36mTextBlob.words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39m@cached_property\u001b[39m\n\u001b[1;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39m    If you want to include punctuation characters, access the ``tokens``\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39m    property.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[39m    :returns: A :class:`WordList <WordList>` of word tokens.\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     \u001b[39mreturn\u001b[39;00m WordList(word_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw, include_punc\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/blob.py:233\u001b[0m, in \u001b[0;36mWordList.__init__\u001b[0;34m(self, collection)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, collection):\n\u001b[1;32m    230\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize a WordList. Takes a collection of strings as\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m    its only argument.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     \u001b[39msuper\u001b[39m(WordList, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m([Word(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m collection])\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/blob.py:233\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, collection):\n\u001b[1;32m    230\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize a WordList. Takes a collection of strings as\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m    its only argument.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     \u001b[39msuper\u001b[39m(WordList, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m([Word(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m collection])\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/tokenizers.py:71\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, include_punc\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convenience function for tokenizing text into words.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    tokenized to sentences before being tokenized to words.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     words \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m---> 71\u001b[0m         _word_tokenizer\u001b[39m.\u001b[39;49mitokenize(sentence, include_punc\u001b[39m=\u001b[39;49minclude_punc,\n\u001b[1;32m     72\u001b[0m                                 \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m         \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sent_tokenize(text))\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m words\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/base.py:64\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[0;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mitokenize\u001b[39m(\u001b[39mself\u001b[39m, text, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    :rtype: generator\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m (t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/textblob/tokenizers.py:34\u001b[0m, in \u001b[0;36mWordTokenizer.tokenize\u001b[0;34m(self, text, include_punc)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, include_punc\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Return a list of word tokens.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m    :param text: string of text.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m    :param include_punc: (optional) whether to include punctuation as separate tokens. Default to True.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mtokenize\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m include_punc:\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/nltk/tokenize/destructive.py:183\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    181\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1 \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2 \u001b[39m\u001b[39m\"\u001b[39m, text)\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m regexp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONTRACTIONS3:\n\u001b[0;32m--> 183\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m1 \u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m2 \u001b[39;49m\u001b[39m\"\u001b[39;49m, text)\n\u001b[1;32m    185\u001b[0m \u001b[39m# We are not using CONTRACTIONS4 since\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# they are also commented out in the SED scripts\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# for regexp in self._contractions.CONTRACTIONS4:\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m#     text = regexp.sub(r' \\1 \\2 \\3 ', text)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/re.py:324\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    321\u001b[0m     template \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m sre_parse\u001b[39m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    325\u001b[0m     \u001b[39m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     template \u001b[39m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m template[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(template[\u001b[39m1\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m         \u001b[39m# literal replacement\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "messages = pd.read_csv('data/train.csv', sep='\\t', quoting=csv.QUOTE_NONE, index_col=False)\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('data/validate.csv', sep='\\t', quoting=csv.QUOTE_NONE, index_col=False)\n",
    "valid_bow = bow_transformer.transform(valid['message'])\n",
    "valid_tfidf = tfidf_transformer.transform(valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.961768219832736\n",
      "f1: 0.8333333333333333\n",
      "auc: 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = naive_bayes.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(naive_bayes, \"NB_42\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.970131421744325\n",
      "f1: 0.878048780487805\n",
      "auc: 0.899716748768473\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = logistic_reg.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(logistic_reg, \"Log_reg_42\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vec = SVC().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.985663082437276\n",
      "f1: 0.9439252336448598\n",
      "auc: 0.9502032019704434\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = support_vec.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(support_vec, \"SVM_42\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For dataset generated by seed = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: invalid gitfile format: data.dvc\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n"
     ]
    }
   ],
   "source": [
    "!git pull data.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('data/train.csv', sep='\\t', quoting=csv.QUOTE_NONE, index_col=False)\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('data/validate.csv', sep='\\t', quoting=csv.QUOTE_NONE, index_col=False)\n",
    "valid_bow = bow_transformer.transform(valid['message'])\n",
    "valid_tfidf = tfidf_transformer.transform(valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.961768219832736\n",
      "f1: 0.8333333333333333\n",
      "auc: 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = naive_bayes.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(naive_bayes, \"NB_42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.970131421744325\n",
      "f1: 0.878048780487805\n",
      "auc: 0.899716748768473\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = logistic_reg.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(logistic_reg, \"Log_reg_42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vec = SVC().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.985663082437276\n",
      "f1: 0.9439252336448598\n",
      "auc: 0.9502032019704434\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    all_predictions = support_vec.predict(valid_tfidf)\n",
    "    valid_metrics = eval_metrics(valid['label'], all_predictions)\n",
    "    mlflow.log_metric(\"acc\", valid_metrics[0])\n",
    "    mlflow.log_metric(\"f1\",valid_metrics[1])\n",
    "    mlflow.log_metric(\"auc\", valid_metrics[2])\n",
    "    print(\"acc:\", valid_metrics[0])\n",
    "    print(\"f1:\",valid_metrics[1])\n",
    "    print(\"auc:\", valid_metrics[2])\n",
    "    mlflow.sklearn.log_model(support_vec, \"SVM_42\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7bec44ed14bb32858b54228f2324fcee84dc4166f61814a98859dcb53b53c94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
